import numpy as np
import random




#================================== Define the maze layout ============================================
maze = np.array([
    [0, 0, 0, -1, 0],  # 0: free space, -1: obstacle, 1: goal
    [0, -1, 0, -1, 0],
    [0, 0, 0, 0, 0],
    [-1, 0, -1, 0, -1],
    [0, 0, 0, 1, 0]     # Goal is at (4, 3)
])



#================================= Define hyperparameters ==============================================
alpha = 0.1               # Learning rate
gamma = 0.9               # Discount factor
epsilon = 0.8             # Exploration rate
epsilon_decay = 0.995
num_episodes = 500



#=========================================== Define actions ============================================
actions = {
    0: (-1, 0),  # Up
    1: (1, 0),   # Down
    2: (0, -1),  # Left
    3: (0, 1)    # Right
}



#============================== Initialize Q-table with zeros (size of the maze x number of actions) ==========================================
no_rows, no_cols = maze.shape
q_table = np.zeros((no_rows, no_cols, 4))  # 4 possible actions (up, down, left, right)



#======================================== Define function to check if a position is valid =========================================
def is_valid_position(maze, position):
    x, y = position
    return 0 <= x < no_rows and 0 <= y < no_cols and maze[x, y] != -1



#======================================= Define function to get next position based on action ======================================
def get_next_position(position, action):
    x, y = position
    dx, dy = actions[action]
    next_position = (x + dx, y + dy)
    return next_position if is_valid_position(maze, next_position) else position



#===================================================== Define reward function ====================================================
def get_reward(position):
    if maze[position] == 1:
        return 10  # Goal reward
    elif maze[position] == -1:
        return -1  # Penalty for hitting an obstacle
    else:
        return -0.1  # Small penalty for each step taken



#====================================================== Q-learning algorithm ===================================================
for episode in range(num_episodes):
    position = (0, 0)  # Start position at top-left corner
    done = False
    
    while not done:
        
        #==================== Choose action based on epsilon-greedy policy =============================
        if random.uniform(0, 1) < epsilon:
            action = random.randint(0, 3)  # Explore: random action
        else:
            action = np.argmax(q_table[position[0], position[1]])  # Exploit: best action

        
        #======================================= Take action and observe next state and reward =====================================
        next_position = get_next_position(position, action)
        reward = get_reward(next_position)

        
        #======================================== Update Q-value using Q-learning formula =========================================
        q_table[position[0], position[1], action] += alpha * (reward + gamma * np.max(q_table[next_position[0], next_position[1]]) - q_table[position[0], position[1], action])

        
        #============================================== Update position ================================================
        position = next_position

        
        #=========================================== Check if goal is reached =============================================
        if maze[position] == 1:
            done = True

    
    #======================================================= Decay epsilon to reduce exploration over time =================================
    epsilon *= epsilon_decay

#======================================================= Display Final Q-table =================================================
print("Training completed!")
print("Final Q-table:")
print(q_table)